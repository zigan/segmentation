{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"SegNet Basic with depthwise-separable convolutions\nWe use depthwise separable convolutions to reduce the model's size (number of trainable parameters).\n\nDepthwise Separable convolutions were first introduced by L. Sifre. Rigid-motion scattering for image classiﬁcation, 2014. in their Ph.D. thesis.\n\nDepthwise separable convolutions were made popular by Mobilenet. You can read more about them here: https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport os\nfrom os import path\nimport torchvision\nimport torchvision.transforms as T\nfrom typing import Sequence\nfrom torchvision.transforms import functional as F\nimport numbers\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport torchmetrics as TM\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-28T00:52:31.518612Z","iopub.execute_input":"2023-07-28T00:52:31.519225Z","iopub.status.idle":"2023-07-28T00:52:31.524922Z","shell.execute_reply.started":"2023-07-28T00:52:31.519197Z","shell.execute_reply":"2023-07-28T00:52:31.524080Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Convert a pytorch tensor into a PIL image\nt2img = T.ToPILImage()\n# Convert a PIL image into a pytorch tensor\nimg2t = T.ToTensor()\n\nfrom enum import IntEnum\nclass TrimapClasses(IntEnum):\n    PET = 0 #Pet pixel (segmentation target)\n    BACKGROUND = 1 #background pixel\n    BORDER = 2 #border (ambiguous) region pixel\n\n# Convert a float trimap ({1, 2, 3} / 255.0) into a float tensor with\n# pixel values in the range 0.0 to 1.0 so that the border pixels\n# can be properly displayed.\ndef trimap2f(trimap):\n    return (img2t(trimap) * 255.0 - 1) / 2\n\n# Create a tensor for a segmentation trimap.\n# Input: Float tensor with values in [0.0 .. 1.0]\n# Output: Long tensor with values in {0, 1, 2}\ndef tensor_trimap(t):\n    x = t * 255\n    x = x.to(torch.long)\n    x = x - 1\n    return x\n\ndef args_to_dict(**kwargs):\n    return kwargs\n\n\n# Create a dataset wrapper that allows us to perform custom image augmentations\n# on both the target and label (segmentation mask) images.\n#\n# These custom image augmentations are needed since we want to perform\n# transforms such as:\n# 1. Random horizontal flip\n# 2. Image resize\n#\n# and these operations need to be applied consistently to both the input\n# image as well as the segmentation mask.\nclass OxfordIIITPetsAugmented(torchvision.datasets.OxfordIIITPet):\n    def __init__(\n        self,\n        root: str,\n        split: str,\n        target_types=\"segmentation\",\n        download=False,\n        pre_transform=None,\n        post_transform=None,\n        pre_target_transform=None,\n        post_target_transform=None,\n        common_transform=None,\n    ):\n        super().__init__(\n            root=root,\n            split=split,\n            target_types=target_types,\n            download=download,\n            transform=pre_transform,\n            target_transform=pre_target_transform,\n        )\n        self.post_transform = post_transform\n        self.post_target_transform = post_target_transform\n        self.common_transform = common_transform\n\n    def __len__(self):\n        return super().__len__()\n\n    def __getitem__(self, idx):\n        (input, target) = super().__getitem__(idx)\n        \n        # Common transforms are performed on both the input and the labels\n        # by creating a 4 channel image and running the transform on both.\n        # Then the segmentation mask (4th channel) is separated out.\n        if self.common_transform is not None:\n            both = torch.cat([input, target], dim=0)\n            both = self.common_transform(both)\n            (input, target) = torch.split(both, 3, dim=0)\n        # end if\n        \n        if self.post_transform is not None:\n            input = self.post_transform(input)\n        if self.post_target_transform is not None:\n            target = self.post_target_transform(target)\n\n        return (input, target)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:52:31.531821Z","iopub.execute_input":"2023-07-28T00:52:31.534019Z","iopub.status.idle":"2023-07-28T00:52:31.547188Z","shell.execute_reply.started":"2023-07-28T00:52:31.533981Z","shell.execute_reply":"2023-07-28T00:52:31.546300Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\ndef save_model_checkpoint(model, cp_name):\n    torch.save(model.state_dict(), os.path.join(working_dir, cp_name))\n\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\n\n# Load model from saved checkpoint\ndef load_model_from_checkpoint(model, ckp_path):\n    return model.load_state_dict(\n        torch.load(\n            ckp_path,\n            map_location=get_device(),\n        )\n    )\n\n# Send the Tensor or Model (input argument x) to the right device\n# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n# otherwise send to CPU.\ndef to_device(x):\n    if torch.cuda.is_available():\n        return x.cuda()\n    else:\n        return x.cpu()\n    \ndef get_model_parameters(m):\n    total_params = sum(\n        param.numel() for param in m.parameters()\n    )\n    return total_params\n\ndef print_model_parameters(m):\n    num_model_parameters = get_model_parameters(m)\n    print(f\"The Model has {num_model_parameters/1e6:.2f}M parameters\")\n# end if\n\ndef close_figures():\n    while len(plt.get_fignums()) > 0:\n        plt.close()\n    # end while\n# end def\n\n\n# Simple torchvision compatible transform to send an input tensor\n# to a pre-specified device.\nclass ToDevice(torch.nn.Module):\n    \"\"\"\n    Sends the input object to the device specified in the\n    object's constructor by calling .to(device) on the object.\n    \"\"\"\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n\n    def forward(self, img):\n        return img.to(self.device)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(device={device})\"\n\n\n\n# SegNet ### https://arxiv.org/abs/1511.00561\n# Model definition. We use a SegNet-Basic model with some minor tweaks.\n# Our input images are 128x128.\n# Model definition. We use a SegNet-Basic model with some minor tweaks.\n# Our input images are 128x128.\n# In this model, we use depth-wise-separable convolutions instead of\n# \"regular\" convolutions.\n\nclass DepthwiseSeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, bias=True):\n        super().__init__()\n        # The depthwise conv is basically just a grouped convolution in PyTorch with\n        # the number of distinct groups being the same as the number of input (and output)\n        # channels for that layer.\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, bias=bias, groups=in_channels)\n        # The pointwise convolution stretches across all the output channels using\n        # a 1x1 kernel.\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\nclass DownDSConv2(nn.Module):\n    def __init__(self, chin, chout, kernel_size):\n        super().__init__()\n        self.seq = nn.Sequential(\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n            DepthwiseSeparableConv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n        )\n        self.mp = nn.MaxPool2d(kernel_size=2, return_indices=True)\n        \n    def forward(self, x):\n        y = self.seq(x)\n        pool_shape = y.shape\n        y, indices = self.mp(y)\n        return y, indices, pool_shape\n\nclass DownDSConv3(nn.Module):\n    def __init__(self, chin, chout, kernel_size):\n        super().__init__()\n        self.seq = nn.Sequential(\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n            DepthwiseSeparableConv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n            DepthwiseSeparableConv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n        )\n        self.mp = nn.MaxPool2d(kernel_size=2, return_indices=True)\n        \n    def forward(self, x):\n        y = self.seq(x)\n        pool_shape = y.shape\n        y, indices = self.mp(y)\n        return y, indices, pool_shape\n\nclass UpDSConv2(nn.Module):\n    def __init__(self, chin, chout, kernel_size):\n        super().__init__()\n        self.seq = nn.Sequential(\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chin),\n            nn.ReLU(),\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n        )\n        self.mup = nn.MaxUnpool2d(kernel_size=2)\n        \n    def forward(self, x, indices, output_size):\n        y = self.mup(x, indices, output_size=output_size)\n        y = self.seq(y)\n        return y\n\nclass UpDSConv3(nn.Module):\n    def __init__(self, chin, chout, kernel_size):\n        super().__init__()\n        self.seq = nn.Sequential(\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chin),\n            nn.ReLU(),\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chin),\n            nn.ReLU(),\n            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(chout),\n            nn.ReLU(),\n        )\n        self.mup = nn.MaxUnpool2d(kernel_size=2)\n        \n    def forward(self, x, indices, output_size):\n        y = self.mup(x, indices, output_size=output_size)\n        y = self.seq(y)\n        return y\n\n\nclass ImageSegmentationDSC(torch.nn.Module):\n    def __init__(self, kernel_size):\n        super().__init__()\n        self.out_channels = 3\n        self.bn_input = nn.BatchNorm2d(3)\n        self.dc1 = DownDSConv2(3, 64, kernel_size=kernel_size)\n        self.dc2 = DownDSConv2(64, 128, kernel_size=kernel_size)\n        self.dc3 = DownDSConv3(128, 256, kernel_size=kernel_size)\n        self.dc4 = DownDSConv3(256, 512, kernel_size=kernel_size)\n        # self.dc5 = DownConv3(512, 512, kernel_size=kernel_size)\n        \n        # self.uc5 = UpConv3(512, 512, kernel_size=kernel_size)\n        self.uc4 = UpDSConv3(512, 256, kernel_size=kernel_size)\n        self.uc3 = UpDSConv3(256, 128, kernel_size=kernel_size)\n        self.uc2 = UpDSConv2(128, 64, kernel_size=kernel_size)\n        self.uc1 = UpDSConv2(64, 3, kernel_size=kernel_size)\n        \n    def forward(self, batch: torch.Tensor):\n        x = self.bn_input(batch)\n        # x = batch\n        # SegNet Encoder\n        x, mp1_indices, shape1 = self.dc1(x)\n        x, mp2_indices, shape2 = self.dc2(x)\n        x, mp3_indices, shape3 = self.dc3(x)\n        x, mp4_indices, shape4 = self.dc4(x)\n        # Our images are 128x128 in dimension. If we run 4 max pooling\n        # operations, we are down to 128/16 = 8x8 activations. If we\n        # do another down convolution, we'll be at 4x4 and at that point\n        # in time, we may lose too much spatial information as a result\n        # of the MaxPooling operation, so we stop at 4 down conv\n        # operations.\n        # x, mp5_indices, shape5 = self.dc5(x)\n\n        # SegNet Decoder\n        # x = self.uc5(x, mp5_indices, output_size=shape5)\n        x = self.uc4(x, mp4_indices, output_size=shape4)\n        x = self.uc3(x, mp3_indices, output_size=shape3)\n        x = self.uc2(x, mp2_indices, output_size=shape2)\n        x = self.uc1(x, mp1_indices, output_size=shape1)\n        \n        return x\n    # end def\n# end class\n\n# Define a custom IoU Metric for validating the model.\ndef IoUMetric(pred, gt, softmax=False):\n    # Run softmax if input is logits.\n    if softmax is True:\n        pred = nn.Softmax(dim=1)(pred)\n    # end if\n    \n    # Add the one-hot encoded masks for all 3 output channels\n    # (for all the classes) to a tensor named 'gt' (ground truth).\n    gt = torch.cat([ (gt == i) for i in range(3) ], dim=1)\n    # print(f\"[2] Pred shape: {pred.shape}, gt shape: {gt.shape}\")\n\n    intersection = gt * pred\n    union = gt + pred - intersection\n\n    # Compute the sum over all the dimensions except for the batch dimension.\n    iou = (intersection.sum(dim=(1, 2, 3)) + 0.001) / (union.sum(dim=(1, 2, 3)) + 0.001)\n    \n    # Compute the mean over the batch dimension.\n    return iou.mean()\n\nclass IoULoss(nn.Module):\n    def __init__(self, softmax=False):\n        super().__init__()\n        self.softmax = softmax\n    \n    # pred => Predictions (logits, B, 3, H, W)\n    # gt => Ground Truth Labales (B, 1, H, W)\n    def forward(self, pred, gt):\n        # return 1.0 - IoUMetric(pred, gt, self.softmax)\n        # Compute the negative log loss for stable training.\n        return -(IoUMetric(pred, gt, self.softmax).log())\n    # end def\n# end class\n\n    def test_custom_iou_loss():\n        #               B, C, H, W\n        x = torch.rand((2, 3, 2, 2), requires_grad=True)\n        y = torch.randint(0, 3, (2, 1, 2, 2), dtype=torch.long)\n        z = IoULoss(softmax=True)(x, y)\n        return z\n    # end def\n\n# Train the model for a single epoch\ndef train_model(model, loader, optimizer):\n    to_device(model.train())\n    cel = True\n    if cel:\n        criterion = nn.CrossEntropyLoss(reduction='mean')\n    else:\n        criterion = IoULoss(softmax=True)\n    # end if\n\n    running_loss = 0.0\n    running_samples = 0\n    \n    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n        optimizer.zero_grad()\n        inputs = to_device(inputs)\n        targets = to_device(targets)\n        outputs = model(inputs)\n        \n        # The ground truth labels have a channel dimension (NCHW).\n        # We need to remove it before passing it into\n        # CrossEntropyLoss so that it has shape (NHW) and each element\n        # is a value representing the class of the pixel.\n        if cel:\n            targets = targets.squeeze(dim=1)\n        # end if\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    \n        running_samples += targets.size(0)\n        running_loss += loss.item()\n    # end for\n\n    print(\"Trained {} samples, Loss: {:.4f}\".format(\n        running_samples,\n        running_loss / (batch_idx+1),\n    ))\n# end def\n\n#### helpers for validation\ndef prediction_accuracy(ground_truth_labels, predicted_labels):\n    eq = ground_truth_labels == predicted_labels\n    return eq.sum().item() / predicted_labels.numel()\n    \ndef print_test_dataset_masks(model, test_pets_targets, test_pets_labels, epoch, save_path, show_plot):\n    to_device(model.eval())\n    predictions = model(to_device(test_pets_targets))\n    test_pets_labels = to_device(test_pets_labels)\n    # print(\"Predictions Shape: {}\".format(predictions.shape))\n    pred = nn.Softmax(dim=1)(predictions)\n\n    pred_labels = pred.argmax(dim=1)\n    # Add a value 1 dimension at dim=1\n    pred_labels = pred_labels.unsqueeze(1)\n    # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n    pred_mask = pred_labels.to(torch.float)\n    \n    # accuracy = prediction_accuracy(test_pets_labels, pred_labels)\n    iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n    iou_accuracy = iou(pred_mask, test_pets_labels)\n    pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n    pixel_accuracy = pixel_metric(pred_labels, test_pets_labels)\n    custom_iou = IoUMetric(pred, test_pets_labels)\n    title = f'Epoch: {epoch:02d}, Accuracy[Pixel: {pixel_accuracy:.4f}, IoU: {iou_accuracy:.4f}, Custom IoU: {custom_iou:.4f}]'\n    print(title)\n    # print(f\"Accuracy: {accuracy:.4f}\")\n\n    # Close all previously open figures.\n    close_figures()\n    \n    fig = plt.figure(figsize=(10, 12))\n    fig.suptitle(title, fontsize=12)\n\n    fig.add_subplot(3, 1, 1)\n    plt.imshow(t2img(torchvision.utils.make_grid(test_pets_targets, nrow=7)))\n    plt.axis('off')\n    plt.title(\"Targets\")\n\n    fig.add_subplot(3, 1, 2)\n    plt.imshow(t2img(torchvision.utils.make_grid(test_pets_labels.float() / 2.0, nrow=7)))\n    plt.axis('off')\n    plt.title(\"Ground Truth Labels\")\n\n    fig.add_subplot(3, 1, 3)\n    plt.imshow(t2img(torchvision.utils.make_grid(pred_mask / 2.0, nrow=7)))\n    plt.axis('off')\n    plt.title(\"Predicted Labels\")\n    \n    if save_path is not None:\n        plt.savefig(os.path.join(save_path, f\"epoch_{epoch:02}.png\"), format=\"png\", bbox_inches=\"tight\", pad_inches=0.4)\n    # end if\n    \n    if show_plot is False:\n        close_figures()\n    else:\n        plt.show()\n    # end if\n# end def\n\ndef test_dataset_accuracy(model, loader):\n    to_device(model.eval())\n    iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n    pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n    \n    iou_accuracies = []\n    pixel_accuracies = []\n    custom_iou_accuracies = []\n    \n    print_model_parameters(model)\n\n    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n        inputs = to_device(inputs)\n        targets = to_device(targets)\n        predictions = model(inputs)\n        \n        pred_probabilities = nn.Softmax(dim=1)(predictions)\n        pred_labels = predictions.argmax(dim=1)\n\n        # Add a value 1 dimension at dim=1\n        pred_labels = pred_labels.unsqueeze(1)\n        # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n        pred_mask = pred_labels.to(torch.float)\n\n        iou_accuracy = iou(pred_mask, targets)\n        # pixel_accuracy = pixel_metric(pred_mask, targets)\n        pixel_accuracy = pixel_metric(pred_labels, targets)\n        custom_iou = IoUMetric(pred_probabilities, targets)\n        iou_accuracies.append(iou_accuracy.item())\n        pixel_accuracies.append(pixel_accuracy.item())\n        custom_iou_accuracies.append(custom_iou.item())\n        \n        del inputs\n        del targets\n        del predictions\n    # end for\n    \n    iou_tensor = torch.FloatTensor(iou_accuracies)\n    pixel_tensor = torch.FloatTensor(pixel_accuracies)\n    custom_iou_tensor = torch.FloatTensor(custom_iou_accuracies)\n    \n    print(\"Test Dataset Accuracy\")\n    print(f\"Pixel Accuracy: {pixel_tensor.mean():.4f}, IoU Accuracy: {iou_tensor.mean():.4f}, Custom IoU Accuracy: {custom_iou_tensor.mean():.4f}\")\n\n\n# Define training loop. This will train the model for multiple epochs.\n#\n# epochs: A tuple containing the start epoch (inclusive) and end epoch (exclusive).\n#         The model is trained for [epoch[0] .. epoch[1]) epochs.\n#\ndef train_loop(model, loader, test_data, epochs, optimizer, scheduler, save_path):\n    test_inputs, test_targets = test_data\n    epoch_i, epoch_j = epochs\n    ###\n    dur = []\n    training_start_time = time.time()\n    ####\n    for i in range(epoch_i, epoch_j):\n        t0 = time.time()\n        epoch = i\n        print(f\"Epoch: {i:02d}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n        train_model(model, loader, optimizer)\n        with torch.inference_mode():\n            # Display the plt in the final training epoch.\n            print_test_dataset_masks(model, test_inputs, test_targets, epoch=epoch, save_path=save_path, show_plot=(epoch == epoch_j-1))\n        # end with\n\n        if scheduler is not None:\n            scheduler.step()\n        # end if\n        ###\n        \n        dur.append(time.time() - t0)\n        print()\n        ###\n        print(\"epoch:{:d}, training time:{:.4f}s\".format(epoch,dur[-1]))\n    # end for\n    print('Training finished, took {:.2f}s'.format(time.time() - training_start_time))\n# end def","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:52:31.686564Z","iopub.execute_input":"2023-07-28T00:52:31.686872Z","iopub.status.idle":"2023-07-28T00:52:31.755902Z","shell.execute_reply.started":"2023-07-28T00:52:31.686844Z","shell.execute_reply":"2023-07-28T00:52:31.754932Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n# Set the working (writable) directory.\n# print(\"running \", os.path.abspath(__file__))\nprint(f\"CUDA: {torch.cuda.is_available()}\")\n#working_dir = \"./\"\nworking_dir = \"/kaggle/working/\"\n\n# pets_data = torchvision.datasets.OxfordIIITPet(working_dir)\n# data_loader = torch.utils.data.DataLoader(pets_data,\n#             batch_size=4,\n#             shuffle=True,\n#             num_workers=1)#args.nThreads)\n# print(data_loader)\n\n# Oxford IIIT Pets Segmentation dataset loaded via torchvision.\npets_path_train = os.path.join(working_dir, 'OxfordPets', 'train')\npets_path_test = os.path.join(working_dir, 'OxfordPets', 'test')\npets_train_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_train, split=\"trainval\", target_types=\"segmentation\", download=True)\npets_test_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_test, split=\"test\", target_types=\"segmentation\", download=True)\n\nprint(pets_path_train,'\\t', pets_train_orig,'\\n',pets_path_test,'\\t', pets_test_orig)\n\n# (train_pets_input, train_pets_target) = pets_train_orig[10]\n# train_pets_input.show()\n# train_pets_target.show()\n# Validation: Check if CUDA is available\n\n# Spot check a segmentation mask image after post-processing it\n# via trimap2f().\n# print(train_pets_input,'\\n',train_pets_target)\n# t2img(trimap2f(train_pets_target)).show()\n# train_pets_target.show()\n\n\n\ntransform_dict = args_to_dict(\n    pre_transform=T.ToTensor(),\n    pre_target_transform=T.ToTensor(),\n    common_transform=T.Compose([\n        ToDevice(get_device()),\n        T.Resize((128, 128), interpolation=T.InterpolationMode.NEAREST),\n        # Random Horizontal Flip as data augmentation.\n        T.RandomHorizontalFlip(p=0.5),\n    ]),\n    post_transform=T.Compose([\n        # Color Jitter as data augmentation.\n        T.ColorJitter(contrast=0.3),\n    ]),\n    post_target_transform=T.Compose([\n        T.Lambda(tensor_trimap),\n    ]),\n)\n\n# Create the train and test instances of the data loader for the\n# Oxford IIIT Pets dataset with random augmentations applied.\n# The images are resized to 128x128 squares, so the aspect ratio\n# will be chaged. We use the nearest neighbour resizing algorithm\n# to avoid disturbing the pixel values in the provided segmentation\n# mask.\npets_train = OxfordIIITPetsAugmented(\n    root=pets_path_train,\n    split=\"trainval\",\n    target_types=\"segmentation\",\n    download=False,\n    **transform_dict,\n)\npets_test = OxfordIIITPetsAugmented(\n    root=pets_path_test,\n    split=\"test\",\n    target_types=\"segmentation\",\n    download=False,\n    **transform_dict,\n)\n\npets_train_loader = torch.utils.data.DataLoader(\n    pets_train,\n    batch_size=64,\n    shuffle=True,\n)\npets_test_loader = torch.utils.data.DataLoader(\n    pets_test,\n    batch_size=21,\n    shuffle=True,\n)\n\n(train_pets_inputs, train_pets_targets) = next(iter(pets_train_loader))\n(test_pets_inputs, test_pets_targets) = next(iter(pets_test_loader))\nprint(f\"train inputs:{train_pets_inputs.shape}, train targets:{train_pets_targets.shape}\")\nprint(f\"test inputs: {test_pets_inputs.shape}, test targets:{test_pets_targets.shape}\")\n\n# Let's inspect some of the images.\n# pets_input_grid = torchvision.utils.make_grid(train_pets_inputs, nrow=8)\n# t2img(pets_input_grid).show()\n# pets_input_grid = torchvision.utils.make_grid(test_pets_inputs, nrow=7)\n# t2img(pets_input_grid).show()\n# Let's inspect the segmentation masks corresponding to the images above.\n#\n# When plotting the segmentation mask, we want to convert the tensor\n# into a float tensor with values in the range [0.0 to 1.0]. However, the\n# mask tensor has the values (0, 1, 2), so we divide by 2.0 to normalize.\n# pets_targets_grid = torchvision.utils.make_grid(train_pets_targets / 2.0, nrow=8)\n# t2img(pets_targets_grid).show()\n#                  N  C  H\n# The printed row is the W dimension.\n# print(train_pets_targets[3][0][4])\n# print(train_pets_targets[63][0][127]) #### maximum index each dimension [index in batx] [???] [index in pixel resolution]\n\n# Run the model once on a single input batch to make sure that the model\n# runs as expected and returns a tensor with the expected shape.\nm = ImageSegmentationDSC(kernel_size=3)\nm.eval()\nto_device(m)\nprint(f\"model tensor shape:\\t{m(to_device(train_pets_inputs)).shape}\")\n\n# Check if our helper functions work as expected and if the image\n# is generated as expected.\nsave_path = os.path.join(working_dir, \"segnet_DepthWise_training_progress_images\")\nos.makedirs(save_path, exist_ok=True)\n# print_test_dataset_masks(m, test_pets_inputs, test_pets_targets, epoch=0, save_path=None, show_plot=True)\n# Optimizer and Learning Rate Scheduler.\nto_device(m)\noptimizer = torch.optim.Adam(m.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.7)\n\n# Train our model for 20 epochs, and record the following:\n#\n# 1. Training Loss\n# 2. Test accuracy metrics for a single batch (21 images) of test images. The following\n#    metrics are computed:\n#   2.1. Pixel Accuracy\n#   2.2. IoU Accuracy (weighted)\n#   2.3. Custom IoU Accuracy\n#\n# We also plot the following for each of the 21 images in the validation batch:\n# 1. Input image\n# 2. Ground truth segmentation mask\n# 3. Predicted segmentation mask\n#\n# so that we can visually inspect the model's progres and determine how well the model\n# is doing qualitatively. Note that the validation metrics on the set of 21 images in\n# the validation set is displayed inline in the notebook only for the last training\n# epoch.\n#\n# save_path = os.path.join(working_dir, \"segnet_basic_training_progress_images\")\nfirst_epoch=1\nlast_epoch=25\ntrain_loop(m, pets_train_loader, (test_pets_inputs, test_pets_targets), (first_epoch, last_epoch+1), optimizer, scheduler, save_path)\n# Save the model's checkpoint.\nsave_model_checkpoint(m, f\"pets_segnet_CrossEntropyLoss_LRSchedule_epoch_{first_epoch}_{last_epoch}.pth\")\nprint_model_parameters(m)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:52:31.758535Z","iopub.execute_input":"2023-07-28T00:52:31.758874Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"CUDA: True\n/kaggle/working/OxfordPets/train \t Dataset OxfordIIITPet\n    Number of datapoints: 3680\n    Root location: /kaggle/working/OxfordPets/train \n /kaggle/working/OxfordPets/test \t Dataset OxfordIIITPet\n    Number of datapoints: 3669\n    Root location: /kaggle/working/OxfordPets/test\ntrain inputs:torch.Size([64, 3, 128, 128]), train targets:torch.Size([64, 1, 128, 128])\ntest inputs: torch.Size([21, 3, 128, 128]), test targets:torch.Size([21, 1, 128, 128])\nmodel tensor shape:\ttorch.Size([64, 3, 128, 128])\nEpoch: 01, Learning Rate: 0.001\nTrained 3680 samples, Loss: 0.8802\nEpoch: 01, Accuracy[Pixel: 0.7544, IoU: 0.5328, Custom IoU: 0.3556]\n\nepoch:1, training time:48.4732s\nEpoch: 02, Learning Rate: 0.001\n","output_type":"stream"}]},{"cell_type":"code","source":"# # save_path = os.path.join(working_dir, \"segnet_basic_training_progress_images\")\n# train_loop(m, pets_train_loader, (test_pets_inputs, test_pets_targets), (2,3), optimizer, scheduler, save_path)\n# # Save the model's checkpoint.\n# save_model_checkpoint(m, f\"pets_segnet_basic_CrossEntropyLoss_LRSchedule_2_epochs.pth\")\n# print_model_parameters(m)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}